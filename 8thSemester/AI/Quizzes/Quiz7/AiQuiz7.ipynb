{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LV0Z-0NoGICV"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class Network:\n",
        "    def __init__(self, sizes):\n",
        "        \"\"\"\n",
        "        sizes: list of layer-sizes, e.g. [784, 30, 10]\n",
        "        \"\"\"\n",
        "        self.num_layers = len(sizes)\n",
        "        self.sizes = sizes\n",
        "        # biases for each non-input layer\n",
        "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
        "        # weights connecting layer l-1 to l\n",
        "        self.weights = [np.random.randn(y, x)/np.sqrt(x)\n",
        "                        for x, y in zip(sizes[:-1], sizes[1:])]\n",
        "\n",
        "    def feedforward(self, a):\n",
        "        \"\"\"Return output of network if input is a.\"\"\"\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            a = sigmoid(np.dot(w, a) + b)\n",
        "        return a\n",
        "\n",
        "    def SGD(self, training_data, epochs, mini_batch_size, eta,\n",
        "            test_data=None):\n",
        "        \"\"\"\n",
        "        Train via mini-batch stochastic gradient descent.\n",
        "        training_data: list of (x, y) pairs\n",
        "        \"\"\"\n",
        "        n = len(training_data)\n",
        "        for j in range(epochs):\n",
        "            np.random.shuffle(training_data)\n",
        "            # partition into mini-batches\n",
        "            mini_batches = [\n",
        "                training_data[k:k+mini_batch_size]\n",
        "                for k in range(0, n, mini_batch_size)\n",
        "            ]\n",
        "            for batch in mini_batches:\n",
        "                self.update_mini_batch(batch, eta)\n",
        "            if test_data:\n",
        "                accuracy = self.evaluate(test_data)\n",
        "                print(f\"Epoch {j}: {accuracy} / {len(test_data)}\")\n",
        "            else:\n",
        "                print(f\"Epoch {j} complete\")\n",
        "\n",
        "    def update_mini_batch(self, batch, eta):\n",
        "        \"\"\"Apply one step of gradient descent on a single mini-batch.\"\"\"\n",
        "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "        for x, y in batch:\n",
        "            # backprop returns gradients for this example\n",
        "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
        "            # accumulate\n",
        "            nabla_b = [nb + dnb for nb, dnb in zip(nabla_b,\n",
        "                                                   delta_nabla_b)]\n",
        "            nabla_w = [nw + dnw for nw, dnw in zip(nabla_w,\n",
        "                                                   delta_nabla_w)]\n",
        "        # update parameters\n",
        "        self.weights = [w - (eta/len(batch))*nw\n",
        "                        for w, nw in zip(self.weights, nabla_w)]\n",
        "        self.biases  = [b - (eta/len(batch))*nb\n",
        "                        for b, nb in zip(self.biases,  nabla_b)]\n",
        "\n",
        "    def backprop(self, x, y):\n",
        "        \"\"\"Return (∂C/∂b, ∂C/∂w) for cost on input x with target y.\"\"\"\n",
        "        # forward pass\n",
        "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "        activation = x\n",
        "        activations = [x]            # list of activations layer by layer\n",
        "        zs = []                      # list of z vectors layer by layer\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            z = w.dot(activation) + b\n",
        "            zs.append(z)\n",
        "            activation = sigmoid(z)\n",
        "            activations.append(activation)\n",
        "\n",
        "        # backward pass\n",
        "        # delta for output layer\n",
        "        delta = self.cost_derivative(activations[-1], y) * sigmoid_prime(zs[-1])\n",
        "        nabla_b[-1] = delta\n",
        "        nabla_w[-1] = delta.dot(activations[-2].T)\n",
        "\n",
        "        # l = 2 means second-to-last layer, etc.\n",
        "        for l in range(2, self.num_layers):\n",
        "            z = zs[-l]\n",
        "            sp = sigmoid_prime(z)\n",
        "            delta = self.weights[-l+1].T.dot(delta) * sp\n",
        "            nabla_b[-l] = delta\n",
        "            nabla_w[-l] = delta.dot(activations[-l-1].T)\n",
        "        return (nabla_b, nabla_w)\n",
        "\n",
        "    def evaluate(self, test_data):\n",
        "        \"\"\"Return # of inputs for which the network output is correct.\"\"\"\n",
        "        test_results = [(np.argmax(self.feedforward(x)), y)\n",
        "                        for (x, y) in test_data]\n",
        "        return sum(int(pred == label) for pred, label in test_results)\n",
        "\n",
        "    def cost_derivative(self, output_activations, y):\n",
        "        \"\"\"∂C/∂a for output activations.\"\"\"\n",
        "        return (output_activations - y)\n",
        "\n",
        "# activation funcs\n",
        "def sigmoid(z): return 1.0/(1.0+np.exp(-z))\n",
        "def sigmoid_prime(z): return sigmoid(z)*(1-sigmoid(z))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0blIcGaz5KUe",
        "outputId": "1b5acce9-3404-4796-d289-81dbee045bed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "# load and normalize\n",
        "(train_X, train_y), (test_X, test_y) = mnist.load_data()\n",
        "train_X = train_X.reshape((-1, 784, 1)) / 255.0\n",
        "test_X  = test_X.reshape((-1, 784, 1)) / 255.0\n",
        "\n",
        "# convert labels to one-hot vectors\n",
        "def one_hot(j):\n",
        "    e = np.zeros((10,1))\n",
        "    e[j] = 1.0\n",
        "    return e\n",
        "training_data = list(zip(train_X, [one_hot(y) for y in train_y]))\n",
        "test_data     = list(zip(test_X,  test_y))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNE6oz_qxwGK",
        "outputId": "19e34f4d-252e-4837-805f-b3dd1cd507fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0: 9174 / 10000\n",
            "Epoch 1: 9283 / 10000\n",
            "Epoch 2: 9334 / 10000\n",
            "Epoch 3: 9407 / 10000\n",
            "Epoch 4: 9452 / 10000\n",
            "Epoch 5: 9479 / 10000\n",
            "Epoch 6: 9504 / 10000\n",
            "Epoch 7: 9524 / 10000\n",
            "Epoch 8: 9529 / 10000\n",
            "Epoch 9: 9547 / 10000\n",
            "Epoch 10: 9559 / 10000\n",
            "Epoch 11: 9559 / 10000\n",
            "Epoch 12: 9578 / 10000\n",
            "Epoch 13: 9586 / 10000\n",
            "Epoch 14: 9584 / 10000\n",
            "Epoch 15: 9601 / 10000\n",
            "Epoch 16: 9604 / 10000\n",
            "Epoch 17: 9596 / 10000\n",
            "Epoch 18: 9603 / 10000\n",
            "Epoch 19: 9616 / 10000\n",
            "Epoch 20: 9610 / 10000\n",
            "Epoch 21: 9620 / 10000\n",
            "Epoch 22: 9632 / 10000\n",
            "Epoch 23: 9635 / 10000\n",
            "Epoch 24: 9628 / 10000\n",
            "Epoch 25: 9632 / 10000\n",
            "Epoch 26: 9640 / 10000\n",
            "Epoch 27: 9636 / 10000\n",
            "Epoch 28: 9634 / 10000\n",
            "Epoch 29: 9636 / 10000\n",
            "Epoch 30: 9641 / 10000\n",
            "Epoch 31: 9650 / 10000\n",
            "Epoch 32: 9640 / 10000\n",
            "Epoch 33: 9639 / 10000\n",
            "Epoch 34: 9645 / 10000\n",
            "Epoch 35: 9640 / 10000\n",
            "Epoch 36: 9654 / 10000\n",
            "Epoch 37: 9646 / 10000\n",
            "Epoch 38: 9656 / 10000\n",
            "Epoch 39: 9656 / 10000\n",
            "Epoch 40: 9653 / 10000\n",
            "Epoch 41: 9655 / 10000\n",
            "Epoch 42: 9647 / 10000\n",
            "Epoch 43: 9655 / 10000\n",
            "Epoch 44: 9653 / 10000\n",
            "Epoch 45: 9652 / 10000\n",
            "Epoch 46: 9663 / 10000\n",
            "Epoch 47: 9655 / 10000\n",
            "Epoch 48: 9642 / 10000\n",
            "Epoch 49: 9647 / 10000\n",
            "Epoch 50: 9646 / 10000\n",
            "Epoch 51: 9656 / 10000\n",
            "Epoch 52: 9648 / 10000\n",
            "Epoch 53: 9660 / 10000\n",
            "Epoch 54: 9656 / 10000\n",
            "Epoch 55: 9657 / 10000\n",
            "Epoch 56: 9639 / 10000\n",
            "Epoch 57: 9665 / 10000\n",
            "Epoch 58: 9659 / 10000\n",
            "Epoch 59: 9660 / 10000\n",
            "Epoch 60: 9657 / 10000\n",
            "Epoch 61: 9669 / 10000\n",
            "Epoch 62: 9652 / 10000\n",
            "Epoch 63: 9667 / 10000\n",
            "Epoch 64: 9661 / 10000\n",
            "Epoch 65: 9669 / 10000\n",
            "Epoch 66: 9659 / 10000\n",
            "Epoch 67: 9666 / 10000\n",
            "Epoch 68: 9658 / 10000\n",
            "Epoch 69: 9666 / 10000\n",
            "Epoch 70: 9657 / 10000\n",
            "Epoch 71: 9672 / 10000\n",
            "Epoch 72: 9661 / 10000\n",
            "Epoch 73: 9667 / 10000\n",
            "Epoch 74: 9661 / 10000\n",
            "Epoch 75: 9665 / 10000\n",
            "Epoch 76: 9666 / 10000\n",
            "Epoch 77: 9651 / 10000\n",
            "Epoch 78: 9658 / 10000\n",
            "Epoch 79: 9664 / 10000\n",
            "Epoch 80: 9653 / 10000\n",
            "Epoch 81: 9662 / 10000\n",
            "Epoch 82: 9666 / 10000\n",
            "Epoch 83: 9660 / 10000\n",
            "Epoch 84: 9650 / 10000\n",
            "Epoch 85: 9652 / 10000\n",
            "Epoch 86: 9664 / 10000\n",
            "Epoch 87: 9657 / 10000\n",
            "Epoch 88: 9663 / 10000\n",
            "Epoch 89: 9652 / 10000\n",
            "Epoch 90: 9651 / 10000\n",
            "Epoch 91: 9649 / 10000\n",
            "Epoch 92: 9657 / 10000\n",
            "Epoch 93: 9659 / 10000\n",
            "Epoch 94: 9658 / 10000\n",
            "Epoch 95: 9658 / 10000\n",
            "Epoch 96: 9659 / 10000\n",
            "Epoch 97: 9661 / 10000\n",
            "Epoch 98: 9664 / 10000\n",
            "Epoch 99: 9660 / 10000\n"
          ]
        }
      ],
      "source": [
        "net = Network([784, 30, 10])\n",
        "net.SGD(training_data, epochs=100, mini_batch_size=20, eta=0.5, test_data=test_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJPgrjDwzVof",
        "outputId": "ec42faa5-1b48-45c4-d2af-f16e1acfd349"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test set accuracy: 96.43%\n"
          ]
        }
      ],
      "source": [
        "# number of correct on test set\n",
        "num_correct = net.evaluate(test_data)\n",
        "\n",
        "# total examples\n",
        "n_test = len(test_data)\n",
        "\n",
        "# accuracy as a fraction or percentage\n",
        "accuracy = num_correct / n_test\n",
        "print(f\"Test set accuracy: {accuracy:.2%}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TlV6ctozx0F1"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "# Grab the 'balanced' split (47 classes)\n",
        "(ds_train, ds_test), ds_info = tfds.load(\n",
        "    'emnist/balanced',\n",
        "    split=['train', 'test'],\n",
        "    as_supervised=True,   # returns (image, label)\n",
        "    with_info=True,\n",
        ")\n",
        "\n",
        "def preprocess(image, label):\n",
        "    # TFDS images are 28×28×1, inverted & rotated; fix orientation:\n",
        "    image = tf.transpose(image, [1, 0, 2])\n",
        "    image = tf.cast(image, tf.float32) / 255.0\n",
        "    image = tf.reshape(image, [784, 1])\n",
        "    label = tf.one_hot(label, depth=47)\n",
        "    return image, label\n",
        "\n",
        "batch_size = 20\n",
        "train_ds = ds_train.map(preprocess).batch(batch_size)\n",
        "test_ds  = ds_test.map(preprocess).batch(batch_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cB_7i_zs5dCT",
        "outputId": "b721a3e1-4603-4078-cab8-8d1a41f7dbe0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0: 9140 / 10000\n",
            "Epoch 1: 9260 / 10000\n",
            "Epoch 2: 9326 / 10000\n",
            "Epoch 3: 9363 / 10000\n",
            "Epoch 4: 9400 / 10000\n",
            "Epoch 5: 9431 / 10000\n",
            "Epoch 6: 9448 / 10000\n",
            "Epoch 7: 9475 / 10000\n",
            "Epoch 8: 9481 / 10000\n",
            "Epoch 9: 9487 / 10000\n",
            "Epoch 10: 9494 / 10000\n",
            "Epoch 11: 9503 / 10000\n",
            "Epoch 12: 9521 / 10000\n",
            "Epoch 13: 9513 / 10000\n",
            "Epoch 14: 9538 / 10000\n",
            "Epoch 15: 9517 / 10000\n",
            "Epoch 16: 9537 / 10000\n",
            "Epoch 17: 9537 / 10000\n",
            "Epoch 18: 9554 / 10000\n",
            "Epoch 19: 9540 / 10000\n",
            "Epoch 20: 9547 / 10000\n",
            "Epoch 21: 9549 / 10000\n",
            "Epoch 22: 9565 / 10000\n",
            "Epoch 23: 9556 / 10000\n",
            "Epoch 24: 9569 / 10000\n",
            "Epoch 25: 9569 / 10000\n",
            "Epoch 26: 9542 / 10000\n",
            "Epoch 27: 9565 / 10000\n",
            "Epoch 28: 9567 / 10000\n",
            "Epoch 29: 9573 / 10000\n",
            "Epoch 30: 9578 / 10000\n",
            "Epoch 31: 9566 / 10000\n",
            "Epoch 32: 9563 / 10000\n",
            "Epoch 33: 9576 / 10000\n",
            "Epoch 34: 9570 / 10000\n",
            "Epoch 35: 9578 / 10000\n",
            "Epoch 36: 9576 / 10000\n",
            "Epoch 37: 9571 / 10000\n",
            "Epoch 38: 9578 / 10000\n",
            "Epoch 39: 9590 / 10000\n",
            "Epoch 40: 9576 / 10000\n",
            "Epoch 41: 9576 / 10000\n",
            "Epoch 42: 9591 / 10000\n",
            "Epoch 43: 9599 / 10000\n",
            "Epoch 44: 9583 / 10000\n",
            "Epoch 45: 9585 / 10000\n",
            "Epoch 46: 9581 / 10000\n",
            "Epoch 47: 9584 / 10000\n",
            "Epoch 48: 9586 / 10000\n",
            "Epoch 49: 9586 / 10000\n",
            "Epoch 50: 9591 / 10000\n",
            "Epoch 51: 9587 / 10000\n",
            "Epoch 52: 9581 / 10000\n",
            "Epoch 53: 9603 / 10000\n",
            "Epoch 54: 9598 / 10000\n",
            "Epoch 55: 9585 / 10000\n",
            "Epoch 56: 9593 / 10000\n",
            "Epoch 57: 9583 / 10000\n",
            "Epoch 58: 9587 / 10000\n",
            "Epoch 59: 9587 / 10000\n",
            "Epoch 60: 9599 / 10000\n",
            "Epoch 61: 9599 / 10000\n",
            "Epoch 62: 9596 / 10000\n",
            "Epoch 63: 9590 / 10000\n",
            "Epoch 64: 9591 / 10000\n",
            "Epoch 65: 9593 / 10000\n",
            "Epoch 66: 9598 / 10000\n",
            "Epoch 67: 9592 / 10000\n",
            "Epoch 68: 9600 / 10000\n",
            "Epoch 69: 9608 / 10000\n",
            "Epoch 70: 9594 / 10000\n",
            "Epoch 71: 9596 / 10000\n",
            "Epoch 72: 9590 / 10000\n",
            "Epoch 73: 9595 / 10000\n",
            "Epoch 74: 9590 / 10000\n"
          ]
        }
      ],
      "source": [
        "net2 = Network([784, 30, 10])\n",
        "net2.SGD(training_data, epochs=75, mini_batch_size=20, eta=0.5, test_data=test_data)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHvAiz197T2l",
        "outputId": "bf144fdc-c316-412a-84b1-679c61643ff7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test set accuracy: 95.90%\n"
          ]
        }
      ],
      "source": [
        "# number of correct on test set\n",
        "num_correct = net2.evaluate(test_data)\n",
        "\n",
        "# total examples\n",
        "n_test = len(test_data)\n",
        "\n",
        "# accuracy as a fraction or percentage\n",
        "accuracy = num_correct / n_test\n",
        "print(f\"Test set accuracy: {accuracy:.2%}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bTEXfGU99iad"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}